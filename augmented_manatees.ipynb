{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "YMX72nfhsDTK"
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzrBGB9grpCj",
    "outputId": "f52719fc-d02a-4090-e0b7-4965db0f183f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 11 09:10:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.68       Driver Version: 471.68       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P5    19W /  N/A |   1635MiB /  8192MiB |     36%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3136    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      5448    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A      9924    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     10308    C+G   ....0.11.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     14704    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     15184    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     15224    C+G   ...ming\\Zoom\\bin\\CptHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15568    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15928    C+G   ...\\app-1.0.9002\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     16516    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     17576    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18980    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     19912    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     20224      C   ...rLab\\Anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     20584    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     21744    C+G   ...ropbox\\Client\\Dropbox.exe    N/A      |\n",
      "|    0   N/A  N/A     22816    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     28476    C+G   ...lack\\app-4.18.0\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     31472    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     32800    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     34184    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     34460    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     34908    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "3OYFAZvOsDTL"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install opencv-python==4.1.2.30 --quiet\n",
    "!pip install numba --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {},
    "id": "rFcxziY5sDTN"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import find_objects, binary_fill_holes\n",
    "from scipy.ndimage import generate_binary_structure, label\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Our import functions\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "BPMdqs5xsDTO"
   },
   "source": [
    "---\n",
    "# Intro to segmentation + denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "IJW1nvH7sDTV"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJD9zfvamFC8"
   },
   "outputs": [],
   "source": [
    "# # Define working directory\n",
    "# working_dir = os.getcwd()        \n",
    "# data_path = os.path.join(working_dir,'data')\n",
    "\n",
    "# # Extract names of image files (as strings)\n",
    "# original_names = []\n",
    "# label_names = {'L':[],'R':[]}\n",
    "\n",
    "# for root, dirs, files in os.walk(data_path):\n",
    "#      for file in files:\n",
    "#         with open(os.path.join(root, file), \"r\") as auto:\n",
    "#             if 'label' in root:\n",
    "#                 if '_L' in file:\n",
    "#                     label_names['L'].append(os.path.join(root, file))\n",
    "#                 if '_R' in file:\n",
    "#                     label_names['R'].append(os.path.join(root, file)) \n",
    "                    \n",
    "#             elif 'original' in root:\n",
    "#                 original_names.append(os.path.join(root, file))\n",
    "\n",
    "# original_names.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n",
    "# label_names['L'].sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n",
    "# label_names['R'].sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hG0GRE_xmFC8"
   },
   "outputs": [],
   "source": [
    "# # Load images from image names\n",
    "# original_images = []\n",
    "# label_images = []\n",
    "# filtered_image_names = []\n",
    "\n",
    "# def binarize(img,threshold=100):\n",
    "#     # make all pixels < threshold black\n",
    "#     binarized = img > threshold\n",
    "#     return binarized\n",
    "\n",
    "# def get_label_name(image_name,hemisphere):\n",
    "#     id_s = Path(image_name).name.split('_')[-1]\n",
    "#     s = Path(image_name).name.split('_')[0:5]\n",
    "#     if hemisphere == 'L':\n",
    "#         s = '_'.join(s) + '_L_' + id_s\n",
    "#     elif hemisphere == 'R':\n",
    "#         s = '_'.join(s) + '_R_' + id_s\n",
    "#     return s\n",
    "\n",
    "# blank = 0\n",
    "\n",
    "# for image_name in original_names:\n",
    "#     img = mpimg.imread(image_name)\n",
    "#     if np.max(img) > 10:        \n",
    "#         name_R = get_label_name(image_name,'R')\n",
    "#         name_L = get_label_name(image_name,'L')\n",
    "\n",
    "#         path_L = list(filter(lambda x: name_L in x, label_names['L']))\n",
    "#         path_R = list(filter(lambda x: name_R in x, label_names['R']))\n",
    "        \n",
    "#         if len(path_R) != 0: # if both R and L label images exist          \n",
    "#             img_L = mpimg.imread(path_L[0])\n",
    "#             img_R = mpimg.imread(path_R[0])            \n",
    "#             label_image = binarize(img_L[:,:,0]) + binarize(img_R[:,:,0])\n",
    "                        \n",
    "#             # Crop images and remove any that don't contain hippocampus            \n",
    "#             if np.max(label_image) == 1:\n",
    "#                 label_images.append(label_image[18:215,:])\n",
    "#                 original_images.append(img[18:215,:,0])\n",
    "#                 filtered_image_names.append(image_name)\n",
    "#                 blank = 1\n",
    "\n",
    "#             elif blank == 1:\n",
    "#                 label_images.append(label_image[18:215,:])\n",
    "#                 original_images.append(img[18:215,:,0])\n",
    "#                 filtered_image_names.append(image_name)  \n",
    "#                 blank = 0\n",
    "                                \n",
    "# original_images = np.array(original_images)\n",
    "# label_images = np.array(label_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xx_vrEXIoJ4J"
   },
   "outputs": [],
   "source": [
    "original_images=np.load('C:\\\\Users\\\\Guanghan-WallerLab\\\\Documents\\\\Neuromatch project\\\\images_with_blanks.npy')\n",
    "label_images=np.load('C:\\\\Users\\\\Guanghan-WallerLab\\\\Documents\\\\Neuromatch project\\\\labels_with_blanks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4130, 197, 197), (4130, 197, 197))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_images.shape, label_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xIn4YD62mFC-"
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_images_abs = np.max(label_images,axis=(1,2))\n",
    "\n",
    "images_train, images_test, label_train, label_test = train_test_split(original_images, label_images, \n",
    "                                                    test_size=0.2, random_state=42, stratify=label_images_abs)\n",
    "\n",
    "label_train_abs = np.max(label_train,axis=(1,2))\n",
    "images_train, images_val, label_train, label_val = train_test_split(images_train, label_train, \n",
    "                                                    test_size=0.2, random_state=42, stratify=label_train_abs)\n",
    "\n",
    "images_train = np.expand_dims(images_train, axis=1)\n",
    "images_val = np.expand_dims(images_val, axis=1)\n",
    "images_test = np.expand_dims(images_test, axis=1)\n",
    "\n",
    "label_train = np.expand_dims(label_train, axis=1)\n",
    "label_val = np.expand_dims(label_val, axis=1)\n",
    "label_test = np.expand_dims(label_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g2vDgWLBmFC_"
   },
   "outputs": [],
   "source": [
    "# Normalize images (zero mean, std=1)\n",
    "mean_train = np.mean(images_train)\n",
    "std_train = np.std(images_train)\n",
    "\n",
    "images_train = (images_train - mean_train) / std_train\n",
    "images_val = (images_val - mean_train) / std_train\n",
    "images_test = (images_test - mean_train) / std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btg3wqcUmFC_"
   },
   "outputs": [],
   "source": [
    "# Load images into pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5egNRwUamFDA"
   },
   "outputs": [],
   "source": [
    "# Train U-Net segmentation network\n",
    "\n",
    "# Class imbalance problem:\n",
    "# Since there are many images with no hippocampus, we want to count these images less towards the loss\n",
    "\n",
    "# When calculating the loss on each forward pass:\n",
    "# LOSS = 0.85 * mean(loss(x_with, y_with)) + 0.15 * mean(loss(x_without, y_without))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h2yCSxvumFDA"
   },
   "outputs": [],
   "source": [
    "def convbatchrelu(in_channels, out_channels, sz):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, sz, padding=sz//2),\n",
    "      nn.BatchNorm2d(out_channels, eps=1e-5),\n",
    "      nn.ReLU(inplace=True),\n",
    "      )\n",
    "\n",
    "\n",
    "class convdown(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential()\n",
    "    for t in range(2):\n",
    "      if t == 0:\n",
    "        self.conv.add_module('conv_%d'%t,\n",
    "                             convbatchrelu(in_channels,\n",
    "                                           out_channels,\n",
    "                                           kernel_size))\n",
    "      else:\n",
    "        self.conv.add_module('conv_%d'%t,\n",
    "                             convbatchrelu(out_channels,\n",
    "                                           out_channels,\n",
    "                                           kernel_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv[0](x)\n",
    "    x = self.conv[1](x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class downsample(nn.Module):\n",
    "  def __init__(self, nbase, kernel_size):\n",
    "    super().__init__()\n",
    "    self.down = nn.Sequential()\n",
    "    self.maxpool = nn.MaxPool2d(2, 2)\n",
    "    for n in range(len(nbase) - 1):\n",
    "      self.down.add_module('conv_down_%d'%n,\n",
    "                           convdown(nbase[n],\n",
    "                                    nbase[n + 1],\n",
    "                                    kernel_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    xd = []\n",
    "    for n in range(len(self.down)):\n",
    "      if n > 0:\n",
    "        y = self.maxpool(xd[n - 1])\n",
    "      else:\n",
    "        y = x\n",
    "      xd.append(self.down[n](y))\n",
    "    return xd\n",
    "\n",
    "\n",
    "class convup(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential()\n",
    "    self.conv.add_module('conv_0', convbatchrelu(in_channels,\n",
    "                                                 out_channels,\n",
    "                                                 kernel_size))\n",
    "    self.conv.add_module('conv_1', convbatchrelu(out_channels,\n",
    "                                                 out_channels,\n",
    "                                                 kernel_size))\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    #print(x.shape, y.shape)\n",
    "    x = self.conv[0](x)\n",
    "    x = self.conv[1](x + y)\n",
    "    return x\n",
    "\n",
    "\n",
    "class upsample(nn.Module):\n",
    "  def __init__(self, nbase, kernel_size):\n",
    "    super().__init__()\n",
    "    self.upsampling = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "    self.up = nn.Sequential()\n",
    "    for n in range(len(nbase) - 1 , 0, -1):\n",
    "      self.up.add_module('conv_up_%d'%(n - 1),\n",
    "              convup(nbase[n], nbase[n - 1], kernel_size))\n",
    "\n",
    "  def forward(self, xd):\n",
    "    x = xd[-1]\n",
    "    for n in range(0, len(self.up)):\n",
    "      if n > 0:\n",
    "        x = self.upsampling(x)\n",
    "      x = self.up[n](x, xd[len(xd) - 1 - n])\n",
    "    return x\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self, nbase, nout, kernel_size):\n",
    "    super(Unet, self).__init__()\n",
    "    self.nbase = nbase\n",
    "    self.nout = nout\n",
    "    self.kernel_size = kernel_size\n",
    "    self.downsample = downsample(nbase, kernel_size)\n",
    "    nbaseup = nbase[1:]\n",
    "    nbaseup.append(nbase[-1])\n",
    "    self.upsample = upsample(nbaseup, kernel_size)\n",
    "    self.output = nn.Conv2d(nbase[1], self.nout, kernel_size,\n",
    "                            padding=kernel_size//2)\n",
    "\n",
    "  def forward(self, data):\n",
    "    T0 = self.downsample(data)\n",
    "    T0 = self.upsample(T0)\n",
    "    T0 = self.output(T0)\n",
    "    return T0\n",
    "\n",
    "  def save_model(self, filename):\n",
    "    torch.save(self.state_dict(), filename)\n",
    "\n",
    "  def load_model(self, filename, cpu=False):\n",
    "    if not cpu:\n",
    "      self.load_state_dict(torch.load(filename))\n",
    "    else:\n",
    "      self.__init__(self.nbase,\n",
    "                    self.nout,\n",
    "                    self.kernel_size,\n",
    "                    self.concatenation)\n",
    "\n",
    "      self.load_state_dict(torch.load(filename,\n",
    "                                      map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WV6w9MpcmFDA"
   },
   "outputs": [],
   "source": [
    "def random_rotate_and_resize(X, Y=None, scale_range=0.5, xy=(224, 224),\n",
    "                             do_flip=True):\n",
    "  \"\"\"\n",
    "  Augmentation by random rotation and resizing\n",
    "\n",
    "  X and Y are lists or arrays of length nimg, with dims channels x Ly x Lx (channels optional)\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  X: ND-array, float\n",
    "    list of IMAGE arrays of size [nchan x Ly x Lx] or [Ly x Lx]\n",
    "\n",
    "  Y: ND-array, float or int (optional, default None)\n",
    "    list of MASK arrays of size [nlabels x Ly x Lx] or [Ly x Lx].\n",
    "    ** These labels are nearest neighbor interpolated\n",
    "    ** CHANGE IF USING FLOAT LABELS\n",
    "\n",
    "  scale_range: float (optional, default 1.0)\n",
    "    Range of resizing of images for augmentation. Images are resized by\n",
    "    (1-scale_range/2) + scale_range * np.random.rand()\n",
    "\n",
    "  xy: tuple, int (optional, default (224,224))\n",
    "    size of transformed images to return\n",
    "\n",
    "  do_flip: bool (optional, default True)\n",
    "    whether or not to flip images horizontally\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  imgi: ND-array, float\n",
    "    transformed images in array [nimg x nchan x xy[0] x xy[1]]\n",
    "\n",
    "  lbl: ND-array, float\n",
    "    transformed labels in array [nimg x nchan x xy[0] x xy[1]]\n",
    "\n",
    "  scale: array, float\n",
    "    amount each image was resized by\n",
    "  \"\"\"\n",
    "\n",
    "  scale_range = max(0, min(2, float(scale_range)))\n",
    "  nimg = len(X)\n",
    "  if X[0].ndim > 2:\n",
    "    nchan = X[0].shape[0]\n",
    "  else:\n",
    "    nchan = 1\n",
    "  imgi  = np.zeros((nimg, nchan, xy[0], xy[1]), np.float32)\n",
    "\n",
    "  lbl = []\n",
    "  if Y is not None:\n",
    "    if Y[0].ndim > 2:\n",
    "      nt = Y[0].shape[0]\n",
    "    else:\n",
    "      nt = 1\n",
    "    lbl = np.zeros((nimg, nt, xy[0], xy[1]), Y.dtype)\n",
    "\n",
    "  scale = np.zeros(nimg, np.float32)\n",
    "  for n in range(nimg):\n",
    "    Ly, Lx = X[n].shape[-2:]\n",
    "\n",
    "    # generate random augmentation parameters\n",
    "    flip = np.random.rand() > .5\n",
    "    theta = np.random.rand() * np.pi * 2\n",
    "    scale[n] = (1 - scale_range / 2) + scale_range * np.random.rand()\n",
    "    dxy = np.maximum(0, np.array([Lx*scale[n] - xy[1], Ly * scale[n] - xy[0]]))\n",
    "    dxy = (np.random.rand(2,) - .5) * dxy\n",
    "\n",
    "    # create affine transform\n",
    "    cc = np.array([Lx / 2, Ly / 2])\n",
    "    cc1 = cc - np.array([Lx - xy[1], Ly - xy[0]]) / 2 + dxy\n",
    "    pts1 = np.float32([cc, cc + np.array([1, 0]), cc + np.array([0, 1])])\n",
    "    pts2 = np.float32([cc1,\n",
    "            cc1 + scale[n]*np.array([np.cos(theta), np.sin(theta)]),\n",
    "            cc1 + scale[n]*np.array([np.cos(np.pi/2 + theta),\n",
    "                                     np.sin(np.pi/2 + theta)])])\n",
    "\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "    img = X[n].copy()\n",
    "    if Y is not None:\n",
    "      labels = Y[n].copy()\n",
    "      if labels.ndim < 3:\n",
    "        labels = labels[np.newaxis, :, :]\n",
    "\n",
    "    if flip and do_flip:\n",
    "      img = img[..., ::-1]\n",
    "      if Y is not None:\n",
    "        labels = labels[..., ::-1]\n",
    "\n",
    "    for k in range(nchan):\n",
    "      I = cv2.warpAffine(img[k], M, (xy[1], xy[0]), flags=cv2.INTER_LINEAR)\n",
    "      imgi[n,k] = I\n",
    "\n",
    "    if Y is not None:\n",
    "      for k in range(nt):\n",
    "        # ** nearest neighbor interpolation **\n",
    "        # may need to change for float labels\n",
    "        lbl[n,k] = cv2.warpAffine(labels[k], M, (xy[1],xy[0]),\n",
    "                                  flags=cv2.INTER_NEAREST)\n",
    "\n",
    "  return imgi, lbl, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jOXXQrH3mFDB"
   },
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "nbase = [1, 32, 64, 128, 256]  # number of channels per layer\n",
    "nout = 2  # number of outputs\n",
    "\n",
    "net = Unet(nbase, nout, kernel_size)\n",
    "# put on GPU here if you have it\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.to(device);  # remove semi-colon to see net structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtfeJ9XXmFDB",
    "outputId": "65408f2d-99c6-4467-8d6f-3aa6598e0247"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.32img/s, Val (epoch)=81.4, loss (epoch)=326]\n",
      "Epoch 2/50:   0%|          | 0/2643 [00:00<?, ?img/s, loss (batch)=0.977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving network state at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.74img/s, Val (epoch)=1.08, loss (epoch)=7.14]\n",
      "Epoch 3/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.54img/s, Val (epoch)=1.17, loss (epoch)=3.82]\n",
      "Epoch 4/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.17img/s, Val (epoch)=1.21, loss (epoch)=3.15]\n",
      "Epoch 5/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.78img/s, Val (epoch)=0.725, loss (epoch)=2.71]\n",
      "Epoch 6/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.60img/s, Val (epoch)=0.569, loss (epoch)=2.33]\n",
      "Epoch 7/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.39img/s, Val (epoch)=0.495, loss (epoch)=2.11]\n",
      "Epoch 8/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.39img/s, Val (epoch)=0.434, loss (epoch)=1.9]\n",
      "Epoch 9/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.45img/s, Val (epoch)=0.405, loss (epoch)=1.67]\n",
      "Epoch 10/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.59img/s, Val (epoch)=0.406, loss (epoch)=1.64]\n",
      "Epoch 11/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.60img/s, Val (epoch)=0.371, loss (epoch)=1.44]\n",
      "Epoch 12/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.59img/s, Val (epoch)=0.329, loss (epoch)=1.34]\n",
      "Epoch 13/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.57img/s, Val (epoch)=0.351, loss (epoch)=1.36]\n",
      "Epoch 14/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.67img/s, Val (epoch)=0.301, loss (epoch)=1.23]\n",
      "Epoch 15/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.64img/s, Val (epoch)=0.294, loss (epoch)=1.17]\n",
      "Epoch 16/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.58img/s, Val (epoch)=0.287, loss (epoch)=1.14]\n",
      "Epoch 17/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.52img/s, Val (epoch)=0.283, loss (epoch)=1.13]\n",
      "Epoch 18/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.42img/s, Val (epoch)=0.289, loss (epoch)=1.08]\n",
      "Epoch 19/50: 100%|██████████| 2643/2643 [00:52<00:00, 50.15img/s, Val (epoch)=0.284, loss (epoch)=1.1]\n",
      "Epoch 20/50: 100%|██████████| 2643/2643 [00:52<00:00, 50.45img/s, Val (epoch)=0.262, loss (epoch)=1.07]\n",
      "Epoch 21/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.24img/s, Val (epoch)=0.268, loss (epoch)=1.05]\n",
      "Epoch 22/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.75img/s, Val (epoch)=0.263, loss (epoch)=1.02]\n",
      "Epoch 23/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.43img/s, Val (epoch)=0.27, loss (epoch)=1.03]\n",
      "Epoch 24/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.11img/s, Val (epoch)=0.26, loss (epoch)=1.02]\n",
      "Epoch 25/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.13img/s, Val (epoch)=0.262, loss (epoch)=0.994]\n",
      "Epoch 26/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.29img/s, Val (epoch)=0.255, loss (epoch)=0.998]\n",
      "Epoch 27/50:   0%|          | 0/2643 [00:00<?, ?img/s, loss (batch)=0.00604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving network state at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.48img/s, Val (epoch)=0.25, loss (epoch)=0.986]\n",
      "Epoch 28/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.79img/s, Val (epoch)=0.251, loss (epoch)=0.989]\n",
      "Epoch 29/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.34img/s, Val (epoch)=0.248, loss (epoch)=0.974]\n",
      "Epoch 30/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.66img/s, Val (epoch)=0.247, loss (epoch)=0.976]\n",
      "Epoch 31/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.35img/s, Val (epoch)=0.254, loss (epoch)=0.959]\n",
      "Epoch 32/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.49img/s, Val (epoch)=0.242, loss (epoch)=0.963]\n",
      "Epoch 33/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.08img/s, Val (epoch)=0.249, loss (epoch)=0.945]\n",
      "Epoch 34/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.30img/s, Val (epoch)=0.243, loss (epoch)=0.955]\n",
      "Epoch 35/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.14img/s, Val (epoch)=0.251, loss (epoch)=0.938]\n",
      "Epoch 36/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.19img/s, Val (epoch)=0.243, loss (epoch)=0.949]\n",
      "Epoch 37/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.09img/s, Val (epoch)=0.244, loss (epoch)=0.949]\n",
      "Epoch 38/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.04img/s, Val (epoch)=0.236, loss (epoch)=0.935]\n",
      "Epoch 39/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.09img/s, Val (epoch)=0.241, loss (epoch)=0.921]\n",
      "Epoch 40/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.06img/s, Val (epoch)=0.244, loss (epoch)=0.932]\n",
      "Epoch 41/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.28img/s, Val (epoch)=0.24, loss (epoch)=0.909]\n",
      "Epoch 42/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.30img/s, Val (epoch)=0.232, loss (epoch)=0.91]\n",
      "Epoch 43/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.43img/s, Val (epoch)=0.249, loss (epoch)=0.905]\n",
      "Epoch 44/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.34img/s, Val (epoch)=0.239, loss (epoch)=0.905]\n",
      "Epoch 45/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.35img/s, Val (epoch)=0.243, loss (epoch)=0.909]\n",
      "Epoch 46/50: 100%|██████████| 2643/2643 [00:53<00:00, 48.96img/s, Val (epoch)=0.239, loss (epoch)=0.912]\n",
      "Epoch 47/50: 100%|██████████| 2643/2643 [00:54<00:00, 48.83img/s, Val (epoch)=0.231, loss (epoch)=0.888]\n",
      "Epoch 48/50: 100%|██████████| 2643/2643 [00:53<00:00, 49.17img/s, Val (epoch)=0.242, loss (epoch)=0.897]\n",
      "Epoch 49/50: 100%|██████████| 2643/2643 [00:48<00:00, 54.32img/s, Val (epoch)=0.233, loss (epoch)=0.893]\n",
      "Epoch 50/50: 100%|██████████| 2643/2643 [00:52<00:00, 50.62img/s, val (batch)=0.00419]  "
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# train the network\n",
    "# parameters related to training the network\n",
    "batch_size = 8 # number of images per batch -- amount of required memory\n",
    "              # for training will increase linearly in batchsize\n",
    "### you will want to increase n_epochs!\n",
    "n_epochs = 50  # number of times to cycle through all the data during training\n",
    "learning_rate = 0.1 # initial learning rate\n",
    "weight_decay = 1e-5 # L2 regularization of weights\n",
    "momentum = 0.9 # how much to use previous gradient direction\n",
    "n_epochs_per_save = 25 # how often to save the network\n",
    "val_frac = 0.05 # what fraction of data to use for validation\n",
    "augmentation = True\n",
    "\n",
    "\n",
    "# where to save the network\n",
    "# make sure to clean these out every now and then, as you will run out of space\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "n_train = images_train.shape[0]\n",
    "n_val = images_val.shape[0]\n",
    "\n",
    "# gradient descent flavor\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=weight_decay,\n",
    "                            momentum=0.9)\n",
    "\n",
    "# set learning rate schedule\n",
    "LR = np.linspace(0, learning_rate, 10)\n",
    "if n_epochs > 250:\n",
    "    LR = np.append(LR, learning_rate*np.ones(n_epochs-100))\n",
    "    for i in range(10):\n",
    "        LR = np.append(LR, LR[-1]/2 * np.ones(10))\n",
    "else:\n",
    "    LR = np.append(LR, learning_rate * np.ones(max(0, n_epochs - 10)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# store loss per epoch\n",
    "epoch_losses = np.zeros(n_epochs)\n",
    "epoch_losses[:] = np.nan\n",
    "val_losses = np.zeros(n_epochs)\n",
    "val_losses[:] = np.nan\n",
    "\n",
    "# when we last saved the network\n",
    "saveepoch = None\n",
    "\n",
    "# loop through entire training data set nepochs times\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "  epoch_loss = 0\n",
    "  val_loss=0\n",
    "  iters = 0\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = LR[epoch]\n",
    "  with tqdm.tqdm(total=n_train, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit='img') as pbar:\n",
    "    # loop through each batch in the training data\n",
    "    net.train() # put in train mode (affects batchnorm)\n",
    "    for ibatch in np.arange(0, n_train, batch_size):\n",
    "      # augment the data\n",
    "      inds = np.arange(ibatch, min(n_train, ibatch+batch_size))\n",
    "      if augmentation == True:\n",
    "        imgs, lbls, _ = random_rotate_and_resize(images_train[inds],label_train[inds].astype(int))\n",
    "      else: \n",
    "        imgs = images_train[inds]\n",
    "        lbls = label_train[inds].astype(int)\n",
    "\n",
    "      # transfer to torch + GPU\n",
    "      imgs = torch.from_numpy(imgs).to(device=device)\n",
    "      lbls = torch.from_numpy(lbls).to(device=device)\n",
    "      imgs = imgs.to(dtype=torch.float32)\n",
    "      lbls = lbls.to(dtype=torch.int64)\n",
    "\n",
    "      # compute the loss\n",
    "      y = net(imgs)\n",
    "      loss = criterion(y, lbls[:, 0])\n",
    "      epoch_loss += loss.item()\n",
    "      pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "      # gradient descent\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      #nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "      optimizer.step()\n",
    "      iters+=1\n",
    "      pbar.update(imgs.shape[0])\n",
    "\n",
    "    net.eval()  \n",
    "\n",
    "    for ibatch in np.arange(0,n_val,batch_size):\n",
    "      inds = np.arange(ibatch, min(n_val, ibatch+batch_size))\n",
    "      if augmentation == True:\n",
    "        imgs_val, lbls_val, _ = random_rotate_and_resize(images_val[inds],label_val[inds].astype(int))\n",
    "      imgs_val = torch.from_numpy(imgs_val).to(device=device)\n",
    "      lbls_val = torch.from_numpy(lbls_val).to(device=device)\n",
    "      imgs_val = imgs_val.to(dtype=torch.float32)\n",
    "      lbls_val = lbls_val.to(dtype=torch.int64)  \n",
    "      output = net(imgs_val)\n",
    "      loss = criterion(output,lbls_val[:, 0])\n",
    "      val_loss+=loss.item()\n",
    "      pbar.set_postfix(**{'val (batch)': loss.item()})\n",
    "\n",
    "\n",
    "    epoch_losses[epoch] = epoch_loss\n",
    "    val_losses[epoch] = val_loss\n",
    "    \n",
    "    pbar.set_postfix(**{'loss (epoch)': epoch_loss,'Val (epoch)': val_loss})  #.update('loss (epoch) = %f'%epoch_loss)\n",
    "\n",
    "  # save checkpoint networks every now and then\n",
    "  if epoch % n_epochs_per_save == 0:\n",
    "    print(f\"\\nSaving network state at epoch {epoch+1}\")\n",
    "    saveepoch = epoch\n",
    "    savefile = f\"unet_epoch{saveepoch+1}.pth\"\n",
    "    net.save_model(savefile)\n",
    "print(f\"\\nSaving network state at epoch {epoch+1}\")\n",
    "net.save_model(f\"unet_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tc98VfeCtwHm",
    "outputId": "bd7bbbfb-1811-4d54-bd41-d9ffe2eae973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2, 224, 224]),\n",
       " torch.Size([8, 1, 224, 224]),\n",
       " (8, 1, 224, 224),\n",
       " (2557, 1, 197, 197),\n",
       " (8, 1, 224, 224),\n",
       " array([640, 641, 642, 643, 644, 645, 646, 647]))"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, lbls_val.shape, lbls.shape, images_train.shape, imgs.shape,inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zTWDbTbj5cXr",
    "outputId": "8346b11a-b816-4dc9-e124-cf96fdb7c4f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(epoch_losses, val_losses):\n",
    "    plt.plot(epoch_losses,'r')\n",
    "    plt.plot(val_losses,'b')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim([0,np.max([epoch_losses, val_losses])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGNrg7MXmFDC"
   },
   "outputs": [],
   "source": [
    "# Every epoch: compute validation loss & store it\n",
    "\n",
    "# plot the training loss and the validation loss over epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "7A6abn8ksDTi"
   },
   "source": [
    "## Model architecture (u-net)\n",
    "\n",
    "A u-net is commonly used for biological image segmentation because its shape allows for local and global features to be combined to create highly-precise segmentations.\n",
    "\n",
    "A u-net is shaped like an autoencoder, it has:\n",
    "1. a standard convolutional network with downsampling, like one used for imagenet\n",
    "2. upsampling layers that ultimately return an image at the same size as the input image\n",
    "In addition to these downsampling and upsampling blocks, it has skip connections from the downsampling blocks TO the upsampling blocks, which allows it to propagate more precise local information to the later layers.\n",
    "\n",
    "adapted from [cellpose/resnet_torch.py](https://github.com/MouseLand/cellpose/blob/master/cellpose/resnet_torch.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "0aH49dTysDTo"
   },
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "FhKOhLmcsDTq"
   },
   "source": [
    "### Train the network\n",
    "\n",
    "Here we've implemented code to train the network.\n",
    "\n",
    "Note we probably should be evaluating test performance throughout training -- implement that yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "WEkqIyX4sDTs"
   },
   "source": [
    "### Test performance\n",
    "\n",
    "Let's see how the network performs on a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "7-x26V1AsDTt"
   },
   "outputs": [],
   "source": [
    "# @markdown Padding code for test images\n",
    "\n",
    "def pad_image_ND(img0, div=16, extra=1):\n",
    "  \"\"\" pad image for test-time so that its dimensions are a multiple of 16 (2D or 3D)\n",
    "\n",
    "  Parameters\n",
    "  -------------\n",
    "  img0: ND-array\n",
    "      image of size [nchan (x Lz) x Ly x Lx]\n",
    "  div: int (optional, default 16)\n",
    "\n",
    "  Returns\n",
    "  --------------\n",
    "  I: ND-array\n",
    "      padded image\n",
    "  slices: tuple, int\n",
    "      range of pixels in I corresponding to img0\n",
    "  \"\"\"\n",
    "  Lpad = int(div * np.ceil(img0.shape[-2] / div) - img0.shape[-2])\n",
    "  xpad1 = extra * div//2 + Lpad//2\n",
    "  xpad2 = extra * div//2 + Lpad - Lpad//2\n",
    "  Lpad = int(div * np.ceil(img0.shape[-1] / div) - img0.shape[-1])\n",
    "  ypad1 = extra * div//2 + Lpad//2\n",
    "  ypad2 = extra * div//2 + Lpad - Lpad//2\n",
    "\n",
    "  if img0.ndim > 3:\n",
    "    pads = np.array([[0, 0], [0, 0], [xpad1, xpad2], [ypad1, ypad2]])\n",
    "  else:\n",
    "    pads = np.array([[0, 0], [xpad1, xpad2], [ypad1, ypad2]])\n",
    "\n",
    "  I = np.pad(img0, pads, mode='constant')\n",
    "\n",
    "  Ly, Lx = img0.shape[-2:]\n",
    "  ysub = np.arange(xpad1, xpad1 + Ly)\n",
    "  xsub = np.arange(ypad1, ypad1 + Lx)\n",
    "  slc = [slice(0, img0.shape[n] + 1) for n in range(img0.ndim)]\n",
    "  slc[-3] = slice(0, imgs.shape[-3] + 1)\n",
    "  slc[-2] = slice(ysub[0], ysub[-1] + 1)\n",
    "  slc[-1] = slice(xsub[0], xsub[-1] + 1)\n",
    "  slc = tuple(slc)\n",
    "\n",
    "  return I, slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtvZnI2ymFDD"
   },
   "outputs": [],
   "source": [
    "out.shape[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm_z2DDAmFDD"
   },
   "outputs": [],
   "source": [
    "plt.imshow(out[0,0].detach())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b7Y8n1jmFDD"
   },
   "outputs": [],
   "source": [
    "plt.imshow(out[0,1].detach())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0Qo88zrmFDD"
   },
   "outputs": [],
   "source": [
    "prediction = np.array(out[0].detach())\n",
    "seg = np.argmax(prediction,axis=0)\n",
    "plt.imshow(seg)\n",
    "plt.colorbar()\n",
    "plt.savefig('brain_segmentation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "YyNqdpyXsDTt"
   },
   "outputs": [],
   "source": [
    "# compute results on test images\n",
    "# (note for unet to run correctly we need to pad images to be divisible by 2**(number of layers))\n",
    "image = images_train[0]\n",
    "\n",
    "net.eval()\n",
    "img_padded, slices = pad_image_ND(image, 16)\n",
    "img_padded = torch.tensor(img_padded[:,0:224,0:224]).unsqueeze(0)\n",
    "img_torch = img_padded.to(torch.float32)\n",
    "#img_torch = torch.from_numpy(img_padded).to(device).unsqueeze(0)  # also need to add a first dimension\n",
    "out = net(img_torch)\n",
    "labels = out[0][slices].detach().cpu()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(image[0])\n",
    "plt.colorbar()\n",
    "plt.title('test MRI image')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(labels[0])\n",
    "plt.colorbar()\n",
    "plt.title('segmentation prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSdS4Fa0mFDE"
   },
   "outputs": [],
   "source": [
    "# Test network performance & plot performance metrics\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/classes.html?highlight=metric#module-sklearn.metrics\n",
    "\n",
    "\n",
    "# F-score\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVIF_BDSmFDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq1_D9trmFDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q69wy4bGmFDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pK-GuoKmFDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q-e2O1vmFDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "IymfPXTKsDTv"
   },
   "source": [
    "### Setting threshold for finding cells\n",
    "\n",
    "We have found areas of \"not cell\" and \"cell\". To create an instance segmentation we need to assign each pixel in a cell to a specific cell rather than a general class. To do this, we will need to find a threshold that produces the best segmentations on our validation set. How do we define a good segmentation? We can use a measure called intersection-over-union (IoU) and call a cell a good cell if it overlaps with a ground-truth cell with an IoU greater than some value. We have taken code from [cellpose/metrics.py] to do this. These functions are based on functions from [stardist], another neat algorithm I recommend checking out!\n",
    "\n",
    "This code below computes the average precision (which you want to maximize) for a given threshold. You'll want to try several thresholds and choose one (probably coding up a loop over reasonable thresholds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "UrnkzHQFsDTw"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "# @markdown `fill_holes_and_remove_small_masks` function\n",
    "def fill_holes_and_remove_small_masks(masks, min_size=15):\n",
    "  \"\"\" fill holes in masks (2D/3D) and discard masks smaller than min_size (2D)\n",
    "\n",
    "  fill holes in each mask using scipy.ndimage.morphology.binary_fill_holes\n",
    "\n",
    "  Parameters\n",
    "  ----------------\n",
    "  masks: int, 2D or 3D array\n",
    "      labelled masks, 0=NO masks; 1,2,...=mask labels,\n",
    "      size [Ly x Lx] or [Lz x Ly x Lx]\n",
    "  min_size: int (optional, default 15)\n",
    "      minimum number of pixels per mask, can turn off with -1\n",
    "\n",
    "  Returns\n",
    "  ---------------\n",
    "  masks: int, 2D or 3D array\n",
    "      masks with holes filled and masks smaller than min_size removed,\n",
    "      0=NO masks; 1,2,...=mask labels,\n",
    "      size [Ly x Lx] or [Lz x Ly x Lx]\n",
    "  \"\"\"\n",
    "  slices = find_objects(masks)\n",
    "  j = 0\n",
    "  for i,slc in enumerate(slices):\n",
    "    if slc is not None:\n",
    "      msk = masks[slc] == (i + 1)\n",
    "      npix = msk.sum()\n",
    "      if min_size > 0 and npix < min_size:\n",
    "        masks[slc][msk] = 0\n",
    "      else:\n",
    "        if msk.ndim==3:\n",
    "          for k in range(msk.shape[0]):\n",
    "            msk[k] = binary_fill_holes(msk[k])\n",
    "        else:\n",
    "          msk = binary_fill_holes(msk)\n",
    "        masks[slc][msk] = (j + 1)\n",
    "        j += 1\n",
    "\n",
    "  return masks\n",
    "\n",
    "\n",
    "# @markdown `average_precision` function\n",
    "def average_precision(masks_true, masks_pred, threshold=[0.5, 0.75, 0.9]):\n",
    "  \"\"\" average precision estimation: AP = TP / (TP + FP + FN)\n",
    "\n",
    "  This function is based heavily on the *fast* stardist matching functions\n",
    "  (https://github.com/mpicbg-csbd/stardist/blob/master/stardist/matching.py)\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  masks_true: list of ND-arrays (int)\n",
    "      where 0=NO masks; 1,2... are mask labels\n",
    "  masks_pred: list of ND-arrays (int)\n",
    "      ND-array (int) where 0=NO masks; 1,2... are mask labels\n",
    "\n",
    "  Returns\n",
    "  ------------\n",
    "  ap: array [len(masks_true) x len(threshold)]\n",
    "      average precision at thresholds\n",
    "  tp: array [len(masks_true) x len(threshold)]\n",
    "      number of true positives at thresholds\n",
    "  fp: array [len(masks_true) x len(threshold)]\n",
    "      number of false positives at thresholds\n",
    "  fn: array [len(masks_true) x len(threshold)]\n",
    "      number of false negatives at thresholds\n",
    "  \"\"\"\n",
    "  if not isinstance(threshold, list) and not isinstance(threshold, np.ndarray):\n",
    "    threshold = [threshold]\n",
    "  ap  = np.zeros((len(masks_true), len(threshold)), np.float32)\n",
    "  tp  = np.zeros((len(masks_true), len(threshold)), np.float32)\n",
    "  fp  = np.zeros((len(masks_true), len(threshold)), np.float32)\n",
    "  fn  = np.zeros((len(masks_true), len(threshold)), np.float32)\n",
    "  n_true = np.array(list(map(np.max, masks_true)))\n",
    "  n_pred = np.array(list(map(np.max, masks_pred)))\n",
    "  for n in range(len(masks_true)):\n",
    "    #_,mt = np.reshape(np.unique(masks_true[n], return_index=True), masks_pred[n].shape)\n",
    "    if n_pred[n] > 0:\n",
    "      iou = _intersection_over_union(masks_true[n], masks_pred[n])[1:, 1:]\n",
    "      for k,th in enumerate(threshold):\n",
    "        tp[n,k] = _true_positive(iou, th)\n",
    "    fp[n] = n_pred[n] - tp[n]\n",
    "    fn[n] = n_true[n] - tp[n]\n",
    "    ap[n] = tp[n] / (tp[n] + fp[n] + fn[n])\n",
    "\n",
    "  return ap, tp, fp, fn\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _label_overlap(x, y):\n",
    "  \"\"\" fast function to get pixel overlaps between masks in x and y\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  x: ND-array, int\n",
    "      where 0=NO masks; 1,2... are mask labels\n",
    "  y: ND-array, int\n",
    "      where 0=NO masks; 1,2... are mask labels\n",
    "\n",
    "  Returns\n",
    "  ------------\n",
    "  overlap: ND-array, int\n",
    "      matrix of pixel overlaps of size [x.max()+1, y.max()+1]\n",
    "  \"\"\"\n",
    "  x = x.ravel()\n",
    "  y = y.ravel()\n",
    "  overlap = np.zeros((1 + x.max(), 1 + y.max()), dtype=np.uint)\n",
    "  for i in range(len(x)):\n",
    "    overlap[x[i], y[i]] += 1\n",
    "\n",
    "  return overlap\n",
    "\n",
    "\n",
    "def _intersection_over_union(masks_true, masks_pred):\n",
    "  \"\"\" intersection over union of all mask pairs\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  masks_true: ND-array, int\n",
    "      ground truth masks, where 0=NO masks; 1,2... are mask labels\n",
    "  masks_pred: ND-array, int\n",
    "      predicted masks, where 0=NO masks; 1,2... are mask labels\n",
    "\n",
    "  Returns\n",
    "  ------------\n",
    "  iou: ND-array, float\n",
    "      matrix of IOU pairs of size [x.max()+1, y.max()+1]\n",
    "  \"\"\"\n",
    "  overlap = _label_overlap(masks_true, masks_pred)\n",
    "  n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n",
    "  n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n",
    "  iou = overlap / (n_pixels_pred + n_pixels_true - overlap)\n",
    "  iou[np.isnan(iou)] = 0.0\n",
    "\n",
    "  return iou\n",
    "\n",
    "\n",
    "def _true_positive(iou, th):\n",
    "  \"\"\" true positive at threshold th\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  iou: float, ND-array\n",
    "      array of IOU pairs\n",
    "  th: float\n",
    "      threshold on IOU for positive label\n",
    "\n",
    "  Returns\n",
    "  ------------\n",
    "  tp: float\n",
    "      number of true positives at threshold\n",
    "  \"\"\"\n",
    "  n_min = min(iou.shape[0], iou.shape[1])\n",
    "  costs = -(iou >= th).astype(float) - iou / (2 * n_min)\n",
    "  true_ind, pred_ind = linear_sum_assignment(costs)\n",
    "  match_ok = iou[true_ind, pred_ind] >= th\n",
    "  tp = match_ok.sum()\n",
    "\n",
    "  return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE2-Kv2ZmFDE"
   },
   "outputs": [],
   "source": [
    "images_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m75D0PDqmFDF"
   },
   "outputs": [],
   "source": [
    "out[slices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKmYtXqzmFDF"
   },
   "outputs": [],
   "source": [
    "plt.imshow(out[slices][0,0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByrSYsTdmFDF"
   },
   "outputs": [],
   "source": [
    "plt.imshow(out[0,0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHY6WK5KmFDF"
   },
   "outputs": [],
   "source": [
    "labels = out[slices].detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "XnFd7olpsDTx"
   },
   "outputs": [],
   "source": [
    "def get_masks_unet(output, cell_threshold=0, min_size=30):\n",
    "  \"\"\" create masks using NOT CELL probability and CELL probability\n",
    "\n",
    "  min_size: minimum number of pixels in the masks\n",
    "  \"\"\"\n",
    "\n",
    "  cells = (output[1] - output[0]) > cell_threshold\n",
    "  selem = generate_binary_structure(cells.ndim, connectivity=1)\n",
    "  masks, nlabels = label(cells, selem)\n",
    "  shape0 = masks.shape\n",
    "  _,masks = np.unique(masks, return_inverse=True)\n",
    "  masks = np.reshape(masks, shape0)\n",
    "  # fill holes and remove small masks\n",
    "  masks = fill_holes_and_remove_small_masks(masks, min_size=min_size)\n",
    "\n",
    "  return masks.astype(np.uint16)\n",
    "\n",
    "\n",
    "# images_test \n",
    "# label_test\n",
    "\n",
    "image = images_train[0]\n",
    "\n",
    "# net.eval()\n",
    "# img_padded, slices = pad_image_ND(image, 16)\n",
    "# img_padded = torch.tensor(img_padded[:,0:224,0:224]).unsqueeze(0)\n",
    "# img_torch = img_padded.to(torch.float32)\n",
    "# #img_torch = torch.from_numpy(img_padded).to(device).unsqueeze(0)  # also need to add a first dimension\n",
    "# out = net(img_torch)\n",
    "# labels = out[0][slices].detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Run the model\n",
    "net.eval()\n",
    "# (depending on GPU capacity you may need to run this in a loop)\n",
    "\n",
    "test_padded, slices = pad_image_ND(images_test, 16)\n",
    "test_padded = torch.tensor(test_padded[:,:,0:224,0:224])\n",
    "test_padded = test_padded.to(torch.float32)\n",
    "#val_torch = torch.from_numpy(val_padded).to(device)\n",
    "out = net(test_padded)\n",
    "\n",
    "# compute CELL / NOT CELL probability\n",
    "labels = out[slices].detach().cpu().numpy()\n",
    "\n",
    "# create masks from probabilities\n",
    "cell_threshold = 2.5\n",
    "masks = [get_masks_unet(lbl, cell_threshold=cell_threshold) for lbl in labels]\n",
    "\n",
    "# (note this function expects multiple masks)\n",
    "iou_threshold = np.arange(0.5, 1, 0.1)\n",
    "ap = average_precision(val_masks, masks, threshold=iou_threshold)[0]\n",
    "\n",
    "# plot results\n",
    "print(ap[:, 0].mean(axis=0))\n",
    "plt.plot(iou_threshold, ap.mean(axis=0))\n",
    "plt.xlabel('IoU threshold')\n",
    "plt.ylabel('average precision')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "BxPExS2OsDTy"
   },
   "source": [
    "Once you choose a threshold, you'll want to use it on your test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "YXRyK3bnsDTz"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# (depending on GPU capacity you may need to run this in a loop)\n",
    "test_padded, slices = pad_image_ND(imgs_test, 8)\n",
    "test_torch = torch.from_numpy(test_padded).to(device)\n",
    "out = net(test_torch)\n",
    "# compute CELL / NOT CELL probability\n",
    "labels = out[slices].detach().cpu().numpy()\n",
    "\n",
    "# create masks from probabilities\n",
    "masks = [get_masks_unet(lbl, cell_threshold=cell_threshold) for lbl in labels]\n",
    "\n",
    "# (note this function expects multiple masks)\n",
    "iou_threshold = np.arange(0.5, 1, 0.1)\n",
    "ap = average_precision(masks_test, masks, threshold=iou_threshold)[0]\n",
    "\n",
    "# plot results\n",
    "print(ap[:,0].mean(axis=0))\n",
    "plt.plot(iou_threshold, ap.mean(axis=0))\n",
    "plt.xlabel('IoU threshold')\n",
    "plt.ylabel('average precision')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "RNvVaQH3sDT0"
   },
   "source": [
    "What kinds of errors is the network making?\n",
    "\n",
    "U-nets with this type of prediction (CELL/NOT CELL) typically overmerge cells. You may see some examples below. In the text at the beginning, ways to avoid this problem are discussed and also one instance (distance to boundary) is implemented in the cellpose repository. \n",
    "\n",
    "You can also compare your results to cellpose using the web interface at [www.cellpose.org](https://www.cellpose.org).\n",
    "\n",
    "Below you can see that we are plotting the ground truth masks (the true masks) and the masks that the algorithm predicted. It may be sort of hard to compare the masks in a jupyter-notebook. One useful tool to visualize imaging data is [napari](https://www.napari.org). You can try running it on your local computer and visualizing your predictions overlaid on the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "m2boWrHUsDT0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(masks_test[0])\n",
    "plt.title('ground truth masks')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(masks[0])\n",
    "plt.title('predicted masks')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "augmented-manatees.ipynb",
   "provenance": []
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
