{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# Setup","metadata":{"execution":{},"id":"YMX72nfhsDTK"}},{"cell_type":"code","source":"# @title Install dependencies\n!pip install opencv-python==4.1.2.30 --quiet\n!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"cellView":"form","id":"3OYFAZvOsDTL","execution":{"iopub.status.busy":"2021-08-20T00:47:27.952293Z","iopub.execute_input":"2021-08-20T00:47:27.952683Z","iopub.status.idle":"2021-08-20T00:49:23.80378Z","shell.execute_reply.started":"2021-08-20T00:47:27.952601Z","shell.execute_reply":"2021-08-20T00:49:23.802771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport os\nimport cv2\nimport tqdm\nimport hashlib\nimport requests\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\nfrom numba import jit\n\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.ndimage import find_objects, binary_fill_holes\nfrom scipy.ndimage import generate_binary_structure, label\nfrom scipy.optimize import linear_sum_assignment\n\n# Our import functions\nimport glob\nimport matplotlib.image as mpimg\nfrom pathlib import Path","metadata":{"id":"rFcxziY5sDTN","execution":{"iopub.status.busy":"2021-08-20T00:49:23.805569Z","iopub.execute_input":"2021-08-20T00:49:23.805926Z","iopub.status.idle":"2021-08-20T00:49:25.661735Z","shell.execute_reply.started":"2021-08-20T00:49:23.805884Z","shell.execute_reply":"2021-08-20T00:49:25.660845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:49:59.868116Z","iopub.execute_input":"2021-08-20T00:49:59.868471Z","iopub.status.idle":"2021-08-20T00:49:59.899425Z","shell.execute_reply.started":"2021-08-20T00:49:59.86844Z","shell.execute_reply":"2021-08-20T00:49:59.898464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_images=np.load('/kaggle/input/hippocampus-segmentation/images_with_blanks.npy')\nlabel_images=np.load('/kaggle/input/hippocampus-segmentation/labels_with_blanks.npy')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:50:00.593267Z","iopub.execute_input":"2021-08-20T00:50:00.593614Z","iopub.status.idle":"2021-08-20T00:50:04.654961Z","shell.execute_reply.started":"2021-08-20T00:50:00.593585Z","shell.execute_reply":"2021-08-20T00:50:04.654089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_images.shape, label_images.shape","metadata":{"id":"UM3ME7t5xk8K","outputId":"8fbd24cd-64ac-46ae-bf94-cbe25dab4f0b","execution":{"iopub.status.busy":"2021-08-20T00:50:04.658572Z","iopub.execute_input":"2021-08-20T00:50:04.65883Z","iopub.status.idle":"2021-08-20T00:50:04.669438Z","shell.execute_reply.started":"2021-08-20T00:50:04.658804Z","shell.execute_reply":"2021-08-20T00:50:04.668503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into train and test set\nfrom sklearn.model_selection import train_test_split\n\nlabel_images_abs = np.max(label_images,axis=(1,2))\n\nimages_train, images_test, label_train, label_test = train_test_split(original_images, label_images, \n                                                    test_size=0.2, random_state=42, stratify=label_images_abs)\n\nlabel_train_abs = np.max(label_train,axis=(1,2))\nimages_train, images_val, label_train, label_val = train_test_split(images_train, label_train, \n                                                    test_size=0.2, random_state=42, stratify=label_train_abs)\n\nimages_train = np.expand_dims(images_train, axis=1)\nimages_val = np.expand_dims(images_val, axis=1)\nimages_test = np.expand_dims(images_test, axis=1)\n\nlabel_train = np.expand_dims(label_train, axis=1)\nlabel_val = np.expand_dims(label_val, axis=1)\nlabel_test = np.expand_dims(label_test, axis=1)","metadata":{"id":"xIn4YD62mFC-","execution":{"iopub.status.busy":"2021-08-20T00:50:09.798437Z","iopub.execute_input":"2021-08-20T00:50:09.798765Z","iopub.status.idle":"2021-08-20T00:50:10.407999Z","shell.execute_reply.started":"2021-08-20T00:50:09.798735Z","shell.execute_reply":"2021-08-20T00:50:10.407112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmented images: transform (sharpen/high contrast) the images for training the GAN\n\ndata_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    # transforms.Scale(256),\n    transforms.ToTensor(),\n    transforms.RandomAdjustSharpness(sharpness_factor=10, p=1),\n    transforms.ColorJitter(brightness=2, contrast=0, saturation=0, hue=0)\n    ])\n\nimage_trans = []\nfor i in range(images_train.shape[0]):\n    image_trans.append(data_transforms(torch.tensor(images_train[i][0])).numpy())\nimage_trans = np.array(image_trans)\n\n\nimages_train_aug = np.vstack([images_train,image_trans])\nlabel_train_aug = np.vstack([label_train,label_train])\n\nidx = torch.randperm(images_train_aug.shape[0]) #randomly shuffle the image order (while preserving image-label association)\nimages_train_aug = images_train_aug[idx]\nlabel_train_aug = label_train_aug[idx]\n\nprint(images_train.shape, images_train_aug.shape)\nprint(label_train.shape, label_train_aug.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:50:12.833574Z","iopub.execute_input":"2021-08-20T00:50:12.833904Z","iopub.status.idle":"2021-08-20T00:50:17.920059Z","shell.execute_reply.started":"2021-08-20T00:50:12.833873Z","shell.execute_reply":"2021-08-20T00:50:17.919239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize images (zero mean, std=1)\nmean_train = np.mean(images_train)\nstd_train = np.std(images_train)\n\nimages_train_aug = (images_train_aug - mean_train) / std_train\nimages_train = (images_train - mean_train) / std_train\nimages_val = (images_val - mean_train) / std_train\nimages_test = (images_test - mean_train) / std_train","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:50:17.921583Z","iopub.execute_input":"2021-08-20T00:50:17.921923Z","iopub.status.idle":"2021-08-20T00:50:19.887421Z","shell.execute_reply.started":"2021-08-20T00:50:17.921885Z","shell.execute_reply":"2021-08-20T00:50:19.886519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(images_train_aug[0,0],cmap='gray')\nplt.axis('off')\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:50:19.889101Z","iopub.execute_input":"2021-08-20T00:50:19.889459Z","iopub.status.idle":"2021-08-20T00:50:20.094711Z","shell.execute_reply.started":"2021-08-20T00:50:19.889422Z","shell.execute_reply":"2021-08-20T00:50:20.093842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(label_train_aug[0,0],cmap='gray')\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:50:27.804738Z","iopub.execute_input":"2021-08-20T00:50:27.805083Z","iopub.status.idle":"2021-08-20T00:50:27.879369Z","shell.execute_reply.started":"2021-08-20T00:50:27.80505Z","shell.execute_reply":"2021-08-20T00:50:27.878377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load images into pytorch DataLoader","metadata":{"id":"Btg3wqcUmFC_","execution":{"iopub.status.busy":"2021-08-20T00:50:28.076937Z","iopub.execute_input":"2021-08-20T00:50:28.077282Z","iopub.status.idle":"2021-08-20T00:50:28.081386Z","shell.execute_reply.started":"2021-08-20T00:50:28.077247Z","shell.execute_reply":"2021-08-20T00:50:28.080244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train U-Net segmentation network\n\n# Class imbalance problem:\n# Since there are many images with no hippocampus, we want to count these images less towards the loss\n\n# When calculating the loss on each forward pass:\n# LOSS = 0.85 * mean(loss(x_with, y_with)) + 0.15 * mean(loss(x_without, y_without))","metadata":{"id":"5egNRwUamFDA","execution":{"iopub.status.busy":"2021-08-20T00:50:28.618783Z","iopub.execute_input":"2021-08-20T00:50:28.619112Z","iopub.status.idle":"2021-08-20T00:50:28.629229Z","shell.execute_reply.started":"2021-08-20T00:50:28.619079Z","shell.execute_reply":"2021-08-20T00:50:28.628257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convbatchrelu(in_channels, out_channels, sz):\n  return nn.Sequential(\n      nn.Conv2d(in_channels, out_channels, sz, padding=sz//2),\n      nn.BatchNorm2d(out_channels, eps=1e-5),\n      nn.ReLU(inplace=True),\n      )\n\n\nclass convdown(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size):\n    super().__init__()\n    self.conv = nn.Sequential()\n    for t in range(2):\n      if t == 0:\n        self.conv.add_module('conv_%d'%t,\n                             convbatchrelu(in_channels,\n                                           out_channels,\n                                           kernel_size))\n      else:\n        self.conv.add_module('conv_%d'%t,\n                             convbatchrelu(out_channels,\n                                           out_channels,\n                                           kernel_size))\n\n  def forward(self, x):\n    x = self.conv[0](x)\n    x = self.conv[1](x)\n    return x\n\n\nclass downsample(nn.Module):\n  def __init__(self, nbase, kernel_size):\n    super().__init__()\n    self.down = nn.Sequential()\n    self.maxpool = nn.MaxPool2d(2, 2)\n    for n in range(len(nbase) - 1):\n      self.down.add_module('conv_down_%d'%n,\n                           convdown(nbase[n],\n                                    nbase[n + 1],\n                                    kernel_size))\n\n  def forward(self, x):\n    xd = []\n    for n in range(len(self.down)):\n      if n > 0:\n        y = self.maxpool(xd[n - 1])\n      else:\n        y = x\n      xd.append(self.down[n](y))\n    return xd\n\n\nclass convup(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size):\n    super().__init__()\n    self.conv = nn.Sequential()\n    self.conv.add_module('conv_0', convbatchrelu(in_channels,\n                                                 out_channels,\n                                                 kernel_size))\n    self.conv.add_module('conv_1', convbatchrelu(out_channels,\n                                                 out_channels,\n                                                 kernel_size))\n\n  def forward(self, x, y):\n    #print(x.shape, y.shape)\n    x = self.conv[0](x)\n    x = self.conv[1](x + y)\n    return x\n\n\nclass upsample(nn.Module):\n  def __init__(self, nbase, kernel_size):\n    super().__init__()\n    self.upsampling = nn.Upsample(scale_factor=2, mode='nearest')\n    self.up = nn.Sequential()\n    for n in range(len(nbase) - 1 , 0, -1):\n      self.up.add_module('conv_up_%d'%(n - 1),\n              convup(nbase[n], nbase[n - 1], kernel_size))\n\n  def forward(self, xd):\n    x = xd[-1]\n    for n in range(0, len(self.up)):\n      if n > 0:\n        x = self.upsampling(x)\n      x = self.up[n](x, xd[len(xd) - 1 - n])\n    return x\n\n\nclass Unet(nn.Module):\n  def __init__(self, nbase, nout, kernel_size):\n    super(Unet, self).__init__()\n    self.nbase = nbase\n    self.nout = nout\n    self.kernel_size = kernel_size\n    self.downsample = downsample(nbase, kernel_size)\n    nbaseup = nbase[1:]\n    nbaseup.append(nbase[-1])\n    self.upsample = upsample(nbaseup, kernel_size)\n    self.output = nn.Conv2d(nbase[1], self.nout, kernel_size,\n                            padding=kernel_size//2)\n\n  def forward(self, data):\n    T0 = self.downsample(data)\n    T0 = self.upsample(T0)\n    T0 = self.output(T0)\n    return T0\n\n  def save_model(self, filename):\n    torch.save(self.state_dict(), filename)\n\n  def load_model(self, filename, cpu=False):\n    if not cpu:\n      self.load_state_dict(torch.load(filename))\n    else:\n      self.__init__(self.nbase,\n                    self.nout,\n                    self.kernel_size,\n                    self.concatenation)\n\n      self.load_state_dict(torch.load(filename,\n                                      map_location=torch.device('cpu')))","metadata":{"id":"h2yCSxvumFDA","execution":{"iopub.status.busy":"2021-08-20T00:50:29.057812Z","iopub.execute_input":"2021-08-20T00:50:29.05815Z","iopub.status.idle":"2021-08-20T00:50:29.081109Z","shell.execute_reply.started":"2021-08-20T00:50:29.058118Z","shell.execute_reply":"2021-08-20T00:50:29.080174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(X,xy=(200, 200)):\n    reshape_im = []\n    for n in range(X.shape[0]):\n        reshape_im.append(cv2.resize(X[n,0],xy))\n    reshape_im = np.array(np.expand_dims(reshape_im,axis=1))\n    return reshape_im\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_size = 3\nnbase = [1, 32, 64, 128, 256]  # number of channels per layer\nnout = 2  # number of outputs\n\nnet = Unet(nbase, nout, kernel_size)\n# put on GPU here if you have it\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnet.to(device);  # remove semi-colon to see net structure","metadata":{"id":"jOXXQrH3mFDB","execution":{"iopub.status.busy":"2021-08-20T00:50:30.81201Z","iopub.execute_input":"2021-08-20T00:50:30.812363Z","iopub.status.idle":"2021-08-20T00:50:33.624057Z","shell.execute_reply.started":"2021-08-20T00:50:30.812327Z","shell.execute_reply":"2021-08-20T00:50:33.623168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\n# train the network\n# parameters related to training the network\nbatch_size = 8 # number of images per batch -- amount of required memory\n              # for training will increase linearly in batchsize\n### you will want to increase n_epochs!\nn_epochs = 50  # number of times to cycle through all the data during training\nlearning_rate = 0.1 # initial learning rate\nweight_decay = 1e-5 # L2 regularization of weights\nmomentum = 0.9 # how much to use previous gradient direction\nn_epochs_per_save = 25 # how often to save the network\nval_frac = 0.05 # what fraction of data to use for validation\naugmentation = True\n\n\n# where to save the network\n# make sure to clean these out every now and then, as you will run out of space\nnow = datetime.now()\ntimestamp = now.strftime('%Y%m%dT%H%M%S')\n\nif augmentation == True:\n    n_train = images_train_aug.shape[0]\n    n_val = images_val.shape[0]    \nelse:\n    n_train = images_train.shape[0]\n    n_val = images_val.shape[0]\n\n# gradient descent flavor\noptimizer = torch.optim.SGD(net.parameters(),\n                            lr=learning_rate,\n                            weight_decay=weight_decay,\n                            momentum=0.9)\n\n# set learning rate schedule\nLR = np.linspace(0, learning_rate, 10)\nif n_epochs > 250:\n    LR = np.append(LR, learning_rate*np.ones(n_epochs-100))\n    for i in range(10):\n        LR = np.append(LR, LR[-1]/2 * np.ones(10))\nelse:\n    LR = np.append(LR, learning_rate * np.ones(max(0, n_epochs - 10)))\n\ncriterion = nn.CrossEntropyLoss()\n\n# store loss per epoch\nepoch_losses = np.zeros(n_epochs)\nepoch_losses[:] = np.nan\nval_losses = np.zeros(n_epochs)\nval_losses[:] = np.nan\n\n# when we last saved the network\nsaveepoch = None\n\n# loop through entire training data set nepochs times\nfor epoch in range(n_epochs):\n\n  epoch_loss = 0\n  val_loss=0\n  iters = 0\n  for param_group in optimizer.param_groups:\n    param_group['lr'] = LR[epoch]\n  with tqdm.tqdm(total=n_train, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit='img') as pbar:\n    # loop through each batch in the training data\n    net.train() # put in train mode (affects batchnorm)\n    for ibatch in np.arange(0, n_train, batch_size):\n      # augment the data\n      inds = np.arange(ibatch, min(n_train, ibatch+batch_size))\n      if augmentation == True:\n        imgs = resize(images_train_aug[inds],xy = (200,200))\n        lbls = resize(label_train_aug[inds].astype(float),xy = (200,200)).astype(int)\n      else: \n        imgs = resize(images_train[inds],xy = (200,200))\n        lbls = resize(label_train[inds].astype(float),xy = (200,200)).astype(int)\n\n      # transfer to torch + GPU\n      imgs = torch.from_numpy(imgs).to(device=device)\n      lbls = torch.from_numpy(lbls).to(device=device)\n      imgs = imgs.to(dtype=torch.float32)\n      lbls = lbls.to(dtype=torch.int64)\n\n      # compute the loss\n      y = net(imgs)\n      loss = criterion(y, lbls[:, 0])\n      epoch_loss += loss.item()\n      pbar.set_postfix(**{'loss (batch)': loss.item()})\n      # gradient descent\n      optimizer.zero_grad()\n      loss.backward()\n      #nn.utils.clip_grad_value_(net.parameters(), 0.1)\n      optimizer.step()\n      iters+=1\n      pbar.update(imgs.shape[0])\n\n    net.eval()  \n\n    for ibatch in np.arange(0,n_val,batch_size):\n      inds = np.arange(ibatch, min(n_val, ibatch+batch_size))\n      imgs_val = resize(images_val[inds],xy = (200,200))\n      lbls_val = resize(label_val[inds].astype(float),xy = (200,200)).astype(int)\n      imgs_val = torch.from_numpy(imgs_val).to(device=device)\n      lbls_val = torch.from_numpy(lbls_val).to(device=device)\n      imgs_val = imgs_val.to(dtype=torch.float32)\n      lbls_val = lbls_val.to(dtype=torch.int64)  \n      output = net(imgs_val)\n      loss = criterion(output,lbls_val[:, 0])\n      val_loss+=loss.item()\n      pbar.set_postfix(**{'val (batch)': loss.item()})\n\n  # Setting up mean loss: n_val/batch_size= total number of batches\n    epoch_losses[epoch] = epoch_loss/(n_train/batch_size)\n    val_losses[epoch] = val_loss/(n_val/batch_size)\n    \n    \n    pbar.set_postfix(**{'loss (epoch)': epoch_loss,'Val (epoch)': val_loss})  #.update('loss (epoch) = %f'%epoch_loss)\n\n  # save checkpoint networks every now and then\n  if epoch % n_epochs_per_save == 0:\n    print(f\"\\nSaving network state at epoch {epoch+1}\")\n    saveepoch = epoch\n    savefile = f\"unet_epoch_aug3_{saveepoch+1}.pth\"\n    net.save_model(savefile)\nprint(f\"\\nSaving network state at epoch {epoch+1}\")\nif augmentation:\n    fname = 'unet_epoch_'+str(epoch+1)+'_aug3.pth'\n    np.save('aug3_epoch_loss',epoch_losses)\n    np.save('aug3_val_losses',val_losses)\nelse:\n    fname = 'unet_epoch_'+str(epoch+1)+'_no_augmentation.pth'\n    np.save('no_augmentation_epoch_loss',epoch_losses)\n    np.save('no_augmentation_val_losses',val_losses)    \nnet.save_model(fname)","metadata":{"id":"dtfeJ9XXmFDB","outputId":"65408f2d-99c6-4467-8d6f-3aa6598e0247","execution":{"iopub.status.busy":"2021-08-19T20:11:17.987357Z","iopub.execute_input":"2021-08-19T20:11:17.987681Z","iopub.status.idle":"2021-08-19T20:57:02.773923Z","shell.execute_reply.started":"2021-08-19T20:11:17.987653Z","shell.execute_reply":"2021-08-19T20:57:02.772958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:52:24.383918Z","iopub.execute_input":"2021-08-20T00:52:24.38431Z","iopub.status.idle":"2021-08-20T00:52:24.404543Z","shell.execute_reply.started":"2021-08-20T00:52:24.384271Z","shell.execute_reply":"2021-08-20T00:52:24.403521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = np.load('/kaggle/input/high-contrast-unet/augmentation2_epoch_loss.npy')\nval_loss = np.load('/kaggle/input/high-contrast-unet/augmentation2_val_losses.npy')\nprint(train_loss.shape, val_loss.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T01:00:06.347494Z","iopub.execute_input":"2021-08-20T01:00:06.347843Z","iopub.status.idle":"2021-08-20T01:00:06.366261Z","shell.execute_reply.started":"2021-08-20T01:00:06.347812Z","shell.execute_reply":"2021-08-20T01:00:06.365389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams.update({'font.size': 18})\n\ndef plot_loss(epoch_losses, val_losses):\n    fig = plt.figure(figsize=(6,6))\n    plt.plot(epoch_losses,'black',label='Training')\n    plt.plot(val_losses,color='r',linestyle=':',label='Validation')\n    plt.xlabel('# Epoch')\n    plt.ylabel('Loss')\n    plt.ylim([0,np.max([epoch_losses, val_losses])])\n    plt.xlim([0,10])\n    plt.legend()\n    \nplot_loss(train_loss,val_loss)","metadata":{"id":"zTWDbTbj5cXr","outputId":"8346b11a-b816-4dc9-e124-cf96fdb7c4f2","execution":{"iopub.status.busy":"2021-08-20T01:05:58.366846Z","iopub.execute_input":"2021-08-20T01:05:58.367186Z","iopub.status.idle":"2021-08-20T01:05:58.53211Z","shell.execute_reply.started":"2021-08-20T01:05:58.367152Z","shell.execute_reply":"2021-08-20T01:05:58.530987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load saved model from local PC\n# net = Unet(nbase, nout, kernel_size) # build a new model\npath = '/kaggle/input/unet-aug3/unet_epoch_50_aug3.pth'\nnet.load_state_dict(torch.load(path,map_location=torch.device('cpu'))) # assign the saved parameters to the new model","metadata":{"id":"Bd0Vtl7PEpvz","execution":{"iopub.status.busy":"2021-08-20T00:56:28.987108Z","iopub.execute_input":"2021-08-20T00:56:28.987626Z","iopub.status.idle":"2021-08-20T00:56:29.427258Z","shell.execute_reply.started":"2021-08-20T00:56:28.987586Z","shell.execute_reply":"2021-08-20T00:56:29.426448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model architecture (u-net)\n\nA u-net is commonly used for biological image segmentation because its shape allows for local and global features to be combined to create highly-precise segmentations.\n\nA u-net is shaped like an autoencoder, it has:\n1. a standard convolutional network with downsampling, like one used for imagenet\n2. upsampling layers that ultimately return an image at the same size as the input image\nIn addition to these downsampling and upsampling blocks, it has skip connections from the downsampling blocks TO the upsampling blocks, which allows it to propagate more precise local information to the later layers.\n\nadapted from [cellpose/resnet_torch.py](https://github.com/MouseLand/cellpose/blob/master/cellpose/resnet_torch.py).\n","metadata":{"execution":{},"id":"7A6abn8ksDTi"}},{"cell_type":"markdown","source":"### Define the network","metadata":{"execution":{},"id":"0aH49dTysDTo"}},{"cell_type":"markdown","source":"### Train the network\n\nHere we've implemented code to train the network.\n\nNote we probably should be evaluating test performance throughout training -- implement that yourself.","metadata":{"execution":{},"id":"FhKOhLmcsDTq"}},{"cell_type":"markdown","source":"### Test performance\n\nLet's see how the network performs on a test image.","metadata":{"execution":{},"id":"WEkqIyX4sDTs"}},{"cell_type":"code","source":"#torch.cuda.empty_cache()\n# !nvidia-smi","metadata":{"id":"-pGouIrsEpv0","execution":{"iopub.status.busy":"2021-08-20T00:56:34.170415Z","iopub.execute_input":"2021-08-20T00:56:34.170753Z","iopub.status.idle":"2021-08-20T00:56:34.174776Z","shell.execute_reply.started":"2021-08-20T00:56:34.170723Z","shell.execute_reply":"2021-08-20T00:56:34.173842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute results on test images\n# (note for unet to run correctly we need to pad images to be divisible by 2**(number of layers))\n# image = images_train[0]\nn = 5;\nimage = resize(images_test)\nlabel = resize(label_test.astype(float)).astype(int)\nnet.eval()\nimg_torch = torch.tensor(image).to(device)\nimg_torch = img_torch.to(torch.float32)\n#img_torch = torch.from_numpy(img_padded).to(device).unsqueeze(0)  # also need to add a first dimension\nprint(img_torch.shape)\nout = net(img_torch[0:n])\nout[0].shape","metadata":{"id":"YyNqdpyXsDTt","outputId":"93e431f6-4e03-4da5-dd16-be7716868741","execution":{"iopub.status.busy":"2021-08-20T00:56:34.45456Z","iopub.execute_input":"2021-08-20T00:56:34.454878Z","iopub.status.idle":"2021-08-20T00:56:35.973727Z","shell.execute_reply.started":"2021-08-20T00:56:34.454848Z","shell.execute_reply":"2021-08-20T00:56:35.972188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import gridspec\n\ncolor_code = '#0699C8'#'#e377c2'\ncolor_code = '#006666'#'#e377c2'\nlegend_pos = 'lower left'\nfig = plt.figure(figsize=(12,12))\nnrows=5;\nncols=4;\ngs = gridspec.GridSpec(nrows, ncols,figure=fig)\n# gs.update(wspace=0.6, hspace=0.3) # set the spacing between axes. \n\nfor rowIndx in range(nrows):\n    prediction = np.array(out[rowIndx].detach().cpu())\n    seg = np.argmax(prediction,axis=0)\n    for cIndx in range(ncols):\n        ax = fig.add_subplot(gs[rowIndx, cIndx]) \n        ax.axis('off')\n        if cIndx == 0:\n            plt.imshow(image[rowIndx][0],cmap='gray')\n            plt.colorbar(fraction=0.046, pad=0.04)\n            if rowIndx == 0:\n                plt.title('Normalized MRI')\n        if cIndx == 1: \n            plt.imshow(seg, cmap = 'gray')\n            plt.colorbar(fraction=0.046, pad=0.04)\n            if rowIndx == 0:\n                plt.title('Predicted seg.')\n        if cIndx == 2: \n            plt.imshow(label[rowIndx][0], cmap = 'gray')\n            plt.colorbar(fraction=0.046, pad=0.04)\n            if rowIndx == 0:\n                plt.title('True seg.')  \n        temp = label[rowIndx][0] +  2*seg        \n        if cIndx == 3: \n            plt.imshow(temp[50:-80,60:-60], cmap = 'bwr')\n            cbar = plt.colorbar(fraction=0.046, pad=0.04,ticks=[0, 1.5, 3])\n            cbar.ax.set_yticklabels(['Background','No overlap','Overlap'],rotation=0)\n            if rowIndx == 0:\n                plt.title('Overlap')                   \nplt.rc('font', family='sans-serif', serif='Helvetica')\nplt.suptitle('Example segmentation for test data (contrast augmentation)', fontsize = 15)\nplt.tight_layout()\nfig.subplots_adjust(top=0.92)\nplt.savefig('Example_with_contrast_aug.png', dpi=600,bbox_inches='tight')\n# plt.savefig('Example_with_augmentation.svg', dpi=600,bbox_inches='tight')","metadata":{"id":"Phj-3YjUEpv1","outputId":"cfd11e96-ec96-4f92-ca83-cb37a8c60d8a","execution":{"iopub.status.busy":"2021-08-20T00:56:37.288178Z","iopub.execute_input":"2021-08-20T00:56:37.288509Z","iopub.status.idle":"2021-08-20T00:56:45.655741Z","shell.execute_reply.started":"2021-08-20T00:56:37.288478Z","shell.execute_reply":"2021-08-20T00:56:45.650719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=12\nplt.imshow(label_train_aug[13,0],cmap='gray')\nplt.axis('off')\nplt.savefig('Example_seg.png', dpi=600,bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:57:00.710879Z","iopub.execute_input":"2021-08-20T00:57:00.71125Z","iopub.status.idle":"2021-08-20T00:57:01.305741Z","shell.execute_reply.started":"2021-08-20T00:57:00.711193Z","shell.execute_reply":"2021-08-20T00:57:01.304726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(images_train_aug[23,0],cmap='gray')\nplt.axis('off')\nplt.savefig('Example_im.png', dpi=600,bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:57:02.187291Z","iopub.execute_input":"2021-08-20T00:57:02.187618Z","iopub.status.idle":"2021-08-20T00:57:02.726916Z","shell.execute_reply.started":"2021-08-20T00:57:02.187587Z","shell.execute_reply":"2021-08-20T00:57:02.725931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = np.array(out.detach().cpu())\nseg = np.argmax(prediction,axis=1)\nplt.imshow(seg[4])","metadata":{"id":"4JwjfmdpSheG","outputId":"16a86f13-9d92-493d-8162-ca7f675118d1","execution":{"iopub.status.busy":"2021-08-20T00:57:05.195592Z","iopub.execute_input":"2021-08-20T00:57:05.195916Z","iopub.status.idle":"2021-08-20T00:57:05.337549Z","shell.execute_reply.started":"2021-08-20T00:57:05.195885Z","shell.execute_reply":"2021-08-20T00:57:05.336576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test network performance & plot performance metrics\n# https://scikit-learn.org/stable/modules/classes.html?highlight=metric#module-sklearn.metrics\nfrom sklearn.metrics import precision_score, accuracy_score, f1_score\n\ndef compute_metrics(all_labels, all_segs): \n    new_label = np.squeeze(all_labels)\n    new_segs = np.squeeze(all_segs)\n    p = []\n    f = []\n    n = new_label.shape[0]\n    for imgIndx in range(n):\n        precision = precision_score(new_label[imgIndx].flatten(), new_segs[imgIndx].flatten())\n        f_score = f1_score(new_label[imgIndx].flatten(), new_segs[imgIndx].flatten())\n        if (np.max(new_label[imgIndx])==0) & (np.max(new_segs[imgIndx])==0):\n          p.append(1.0)\n          f.append(1.0)\n        else:\n          p.append(precision)\n          f.append(f_score)     \n    return p, f","metadata":{"id":"x4KeG-ngLS23","execution":{"iopub.status.busy":"2021-08-20T00:57:08.559593Z","iopub.execute_input":"2021-08-20T00:57:08.559927Z","iopub.status.idle":"2021-08-20T00:57:08.567872Z","shell.execute_reply.started":"2021-08-20T00:57:08.559896Z","shell.execute_reply":"2021-08-20T00:57:08.566923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart = time.time()\nnet.eval()\nimage = resize(images_test)\nlabel = resize(label_test.astype(float)).astype(int)\nbatch_size = 16\nseg = np.empty((label.shape[0], label.shape[2],label.shape[3]));              # initialize the predictions\nn_test = images_test.shape[0]\nfor ibatch in np.arange(0,n_test,batch_size):\n    inds = np.arange(ibatch, min(n_test, ibatch+batch_size))\n    img_torch = torch.tensor(image[inds]).to(device)\n    img_torch = img_torch.to(torch.float32)\n    lbls_torch = torch.tensor(label[inds]).to(device)\n    #     lbls_torch = lbls_torch.to(torch.int64)\n    out = net(img_torch)\n    out = np.array((out.detach()).to('cpu'))\n    \n\n    out = np.argmax(out,axis=1)\n#     print(out.shape)\n#     seg[inds][:][:][:] = np.expand_dims(out, 1)      #I don't know hwy this does not work..\n    seg[inds] = out \n#     #     print(np.expand_dims(out, 1).shape)\nseg = np.expand_dims(seg, 1)\nend = time.time()\nprint(end - start)","metadata":{"id":"L6NULGA-erb3","outputId":"00181a58-95b9-494c-e04d-d316678e9e21","execution":{"iopub.status.busy":"2021-08-20T00:57:12.02564Z","iopub.execute_input":"2021-08-20T00:57:12.025984Z","iopub.status.idle":"2021-08-20T00:57:16.12368Z","shell.execute_reply.started":"2021-08-20T00:57:12.025953Z","shell.execute_reply":"2021-08-20T00:57:16.12273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_label_test = resize(label_test.astype(float)).astype(int)\np, f = compute_metrics(np.squeeze(resize_label_test), seg)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T00:57:16.125117Z","iopub.execute_input":"2021-08-20T00:57:16.125645Z","iopub.status.idle":"2021-08-20T00:57:39.697757Z","shell.execute_reply.started":"2021-08-20T00:57:16.125603Z","shell.execute_reply":"2021-08-20T00:57:39.69688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean of p: ', np.mean(p) )\nprint('Median of p: ', np.median(p) )\nprint('STD of p: ', np.std(p) )\nprint('Mean of F1: ', np.mean(f) )\nprint('Median of F1: ', np.median(f) )\nprint('STD of F1: ', np.std(f) )","metadata":{"id":"Wd6vG8t0rAjF","outputId":"6005e956-61ef-4c65-9c73-8f926a275be3","execution":{"iopub.status.busy":"2021-08-20T00:57:39.699299Z","iopub.execute_input":"2021-08-20T00:57:39.699637Z","iopub.status.idle":"2021-08-20T00:57:39.711328Z","shell.execute_reply.started":"2021-08-20T00:57:39.699606Z","shell.execute_reply":"2021-08-20T00:57:39.709281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot precision and F1 score\nplt.figure(figsize=(2.5,2.5))\nplt.hist(p,20)\nplt.vlines(np.median(p),0,200,color='black')\nplt.xlabel('Prediction precision')\nplt.ylabel('Counts')\nplt.suptitle('High-contrast aug.')\n\nplt.savefig('Highcontrast_aug_precision.png', dpi=600,bbox_inches='tight')\n\nplt.figure(figsize=(2.5,2.5))\nplt.hist(f,20)\nplt.vlines(np.median(f),0,300,color='black')\nplt.xlabel('Prediction F1 score')\nplt.ylabel('Counts')\nplt.suptitle('High-contrast aug.')\n\nplt.savefig('Highcontrast_aug_f1.png', dpi=600,bbox_inches='tight')\n\n","metadata":{"id":"Vhh30qG4Zxf7","outputId":"02a04cc1-e1e1-4581-c274-a55f57fbe6d0","execution":{"iopub.status.busy":"2021-08-20T00:57:39.712822Z","iopub.execute_input":"2021-08-20T00:57:39.71315Z","iopub.status.idle":"2021-08-20T00:57:40.417892Z","shell.execute_reply.started":"2021-08-20T00:57:39.713115Z","shell.execute_reply":"2021-08-20T00:57:40.416928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = {}\nmedian = {}\nSTD = {}\n\n# augmentation with contrast\nmean['con_p'] = np.mean(p)\nmedian['con_p'] = np.median(p)\nSTD['con_p'] = np.std(p)\nmean['con_f'] = np.mean(f)\nmedian['con_f'] = np.median(f)\nSTD['con_f'] = np.std(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(resize_label_test[indx[nn]], seg[indx[nn]])\nprint(average_precision)\n","metadata":{"id":"We7YErYFhoZj","outputId":"2623636a-b0ce-49a2-fc89-058ebd53616b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find images with low prediction precision \nindx = np.where((np.array(p)<0.3) & (np.array(p)>0.2))[0]\nindx","metadata":{"id":"F850y-OLgBWB","outputId":"80e0f4d9-0bfa-4218-e784-a36f44c0c3e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p[indx[nn]]","metadata":{"id":"v2PKpf8GhBaf","outputId":"2fce2215-a272-4867-ac10-d26b965a6e42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_, f_ = compute_metrics(resize_label_test[indx[nn]:indx[nn]+1], seg[indx[nn]:indx[nn]+1])\nprint(p_,f_)\n","metadata":{"id":"VY6-1zKbfvHl","outputId":"5589345c-8c80-4f7f-bc46-77cf90676816"},"execution_count":null,"outputs":[]}]}